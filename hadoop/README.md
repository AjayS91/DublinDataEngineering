# Introduction to Hadoop

The Apache Hadoop  is an open-source software framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models (MapReduce). It is designed to scale up from single servers to thousands of machines, each offering local computation and storage.   http://hadoop.apache.org/  

Doug Cutting (one of the founders of Hadoop)

You can build Hadoop stacks yourself downloading from apache Hadoop or download the pre-built software stacks from the below companies. 
Cloudera, Hortonworks and MapR  provide the core pre-built software stacks for free and offer commercial support for production environments. 

[Apache Hadoop ecosystem    over 100 projects.](https://hadoopecosystemtable.github.io/)

![Hadoop Ecosystem](img/hadoopecosystem.png)

